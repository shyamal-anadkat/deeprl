{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "4_callbacks_hyperparameter_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb110a7f43cc4e39ba9872d0e61f9c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f77546bde6cc492d83388baef3c86c71",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e3b9eecaf1a947eba243cace4ae06944",
              "IPY_MODEL_a638279796f34e009997cae87b3dedd3"
            ]
          }
        },
        "f77546bde6cc492d83388baef3c86c71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3b9eecaf1a947eba243cace4ae06944": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5d68639747f44d48a8f7e367791d1753",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4e3c323f13d14c44be6cdf08c2931edb"
          }
        },
        "a638279796f34e009997cae87b3dedd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a75006590f074b6390e17ecb82991155",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2000/2000 [02:15&lt;00:00, 14.77it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a1f2ad0c12d6481a80a62c6af16aeb7c"
          }
        },
        "5d68639747f44d48a8f7e367791d1753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4e3c323f13d14c44be6cdf08c2931edb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a75006590f074b6390e17ecb82991155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a1f2ad0c12d6481a80a62c6af16aeb7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5a9716bf2084a5293d36b0b37377340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e3913d5e71b94527b9075ac7ef3f4be8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3e6d9e5d3630412798bd38e6b520bdc7",
              "IPY_MODEL_6cab26d2a3084ad299d7d1ef3c0e2562"
            ]
          }
        },
        "e3913d5e71b94527b9075ac7ef3f4be8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e6d9e5d3630412798bd38e6b520bdc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c6fb6368319346b082995d4e92f90bca",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 10000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1d73e195517146b18fd6113dde1dee44"
          }
        },
        "6cab26d2a3084ad299d7d1ef3c0e2562": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bdaa7410d3cd4d9e9bcc234ef41defc5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10000/10000 [00:27&lt;00:00, 364.99it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9782cfdd1b6e482c98367413f87b9b73"
          }
        },
        "c6fb6368319346b082995d4e92f90bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1d73e195517146b18fd6113dde1dee44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bdaa7410d3cd4d9e9bcc234ef41defc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9782cfdd1b6e482c98367413f87b9b73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shyamal-anadkat/deeprl/blob/main/hw1/tutorials/4_callbacks_hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8lIXBiHRYb6"
      },
      "source": [
        "# Stable Baselines3 Tutorial - Callbacks and hyperparameter tuning\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn how to use [Callbacks](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html) which allow to do monitoring, auto saving, model manipulation, progress bars, ...\n",
        "\n",
        "\n",
        "You will also see that finding good hyperparameters is key to success in RL.\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owKXXp8rRZI7"
      },
      "source": [
        "!apt install swig\n",
        "!pip install tqdm==4.36.1\n",
        "!pip install stable-baselines3[extra]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18ivrnsaSWUn"
      },
      "source": [
        "import gym\n",
        "from stable_baselines3 import A2C, SAC, PPO, TD3"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PytOtL9GdmrE"
      },
      "source": [
        "# The importance of hyperparameter tuning\n",
        "\n",
        "When compared with Supervised Learning, Deep Reinforcement Learning is far more sensitive to the choice of hyper-parameters such as learning rate, number of neurons, number of layers, optimizer ... etc. \n",
        "Poor choice of hyper-parameters can lead to poor/unstable convergence. This challenge is compounded by the variability in performance across random seeds (used to initialize the network weights and the environment).\n",
        "\n",
        "Here we demonstrate on a toy example the [Soft Actor Critic](https://arxiv.org/abs/1801.01290) algorithm applied in the Pendulum environment. Note the change in performance between the default and \"tuned\" parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5oVvYHwdnYv"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a0v3fgwe54j"
      },
      "source": [
        "eval_env = gym.make('Pendulum-v0')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WRR7kmIeqEB"
      },
      "source": [
        "default_model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1, seed=0, batch_size=64, policy_kwargs=dict(net_arch=[64, 64])).learn(8000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQbDcbEheqWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df1d0155-ca59-4227-e303-2fda23e6e4c7"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(default_model, eval_env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward:-859.20 +/- 57.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smMdkZnvfL1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b73cf07-15bf-49c3-f722-1c950a2f3f42"
      },
      "source": [
        "tuned_model = SAC('MlpPolicy', 'Pendulum-v0', batch_size=256, verbose=1, policy_kwargs=dict(net_arch=[256, 256]), seed=0).learn(8000)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -604     |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 64       |\n",
            "|    time_elapsed    | 124      |\n",
            "|    total_timesteps | 8000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 68.1     |\n",
            "|    critic_loss     | 1.82     |\n",
            "|    ent_coef        | 0.163    |\n",
            "|    ent_coef_loss   | -0.279   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 7899     |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN05_Io8fMAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646d13cb-2f9c-47ae-9c43-cacbeba6de58"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(tuned_model, eval_env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward:-137.48 +/- 87.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi9IwxBYVMl8"
      },
      "source": [
        "Exploring hyperparameter tuning is out of the scope (and time schedule) of this tutorial. However, you need to know that we provide tuned hyperparameter in the [rl zoo](https://github.com/DLR-RM/rl-baselines3-zoo) as well as automatic hyperparameter optimization using [Optuna](https://github.com/pfnet/optuna).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irHk8FXdRUnw"
      },
      "source": [
        "# Callbacks\n",
        "\n",
        "\n",
        "Please read the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html). Although Stable-Baselines3 provides you with a callback collection (e.g. for creating checkpoints or for evaluation), we are going to re-implement some so you can get a good understanding of how they work.\n",
        "\n",
        "To build a custom callback, you need to create a class that derives from `BaseCallback`. This will give you access to events (`_on_training_start`, `_on_step()`) and useful variables (like `self.model` for the RL model).\n",
        "\n",
        "`_on_step` returns a boolean value for whether or not the training should continue.\n",
        "\n",
        "Thanks to the access to the models variables, in particular `self.model`, we are able to even change the parameters of the model without halting the training, or changing the model's code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE30k2i7kohh"
      },
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjRvJ8zBftL3"
      },
      "source": [
        "class CustomCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    A custom callback that derives from ``BaseCallback``.\n",
        "\n",
        "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
        "    \"\"\"\n",
        "    def __init__(self, verbose=0):\n",
        "        super(CustomCallback, self).__init__(verbose)\n",
        "        # Those variables will be accessible in the callback\n",
        "        # (they are defined in the base class)\n",
        "        # The RL model\n",
        "        # self.model = None  # type: BaseRLModel\n",
        "        # An alias for self.model.get_env(), the environment used for training\n",
        "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
        "        # Number of time the callback was called\n",
        "        # self.n_calls = 0  # type: int\n",
        "        # self.num_timesteps = 0  # type: int\n",
        "        # local and global variables\n",
        "        # self.locals = None  # type: Dict[str, Any]\n",
        "        # self.globals = None  # type: Dict[str, Any]\n",
        "        # The logger object, used to report things in the terminal\n",
        "        # self.logger = None  # type: logger.Logger\n",
        "        # # Sometimes, for event callback, it is useful\n",
        "        # # to have access to the parent object\n",
        "        # self.parent = None  # type: Optional[BaseCallback]\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_rollout_start(self) -> None:\n",
        "        \"\"\"\n",
        "        A rollout is the collection of environment interaction\n",
        "        using the current policy.\n",
        "        This event is triggered before collecting new samples.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        \"\"\"\n",
        "        This method will be called by the model after each call to `env.step()`.\n",
        "\n",
        "        For child callback (of an `EventCallback`), this will be called\n",
        "        when the event is triggered.\n",
        "\n",
        "        :return: (bool) If the callback returns False, training is aborted early.\n",
        "        \"\"\"\n",
        "        return True\n",
        "\n",
        "    def _on_rollout_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before updating the policy.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqpPtxaCfynB"
      },
      "source": [
        "Here we have a simple callback that can only be called twice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ILY0AkFfzPJ"
      },
      "source": [
        "class SimpleCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    a simple callback that can only be called twice\n",
        "\n",
        "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
        "    \"\"\"\n",
        "    def __init__(self, verbose=0):\n",
        "        super(SimpleCallback, self).__init__(verbose)\n",
        "        self._called = False\n",
        "    \n",
        "    def _on_step(self):\n",
        "      if not self._called:\n",
        "        print(\"callback - first call\")\n",
        "        self._called = True\n",
        "        return True # returns True, training continues.\n",
        "      print(\"callback - second call\")\n",
        "      return False # returns False, training stops.      "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gTXaNLARUnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "140f9528-bf4b-4bc8-a317-5d3ff698fa10"
      },
      "source": [
        "model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1)\n",
        "model.learn(8000, callback=SimpleCallback())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Creating environment from the given name 'Pendulum-v0'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "callback - first call\n",
            "callback - second call\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.sac.sac.SAC at 0x7f93c8dd3250>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adsKMvDkRUn0"
      },
      "source": [
        "## First example: Auto saving best model\n",
        "In RL, it is quite useful to keep a clean version of a model as you are training, as we can end up with burn-in of a bad policy. This is a typical use case for callback, as they can call the save function of the model, and observe the training over time.\n",
        "\n",
        "Using the monitoring wrapper, we can save statistics of the environment, and use them to determine the mean training reward.\n",
        "This allows us to save the best model while training.\n",
        "\n",
        "Note that this is not the proper way of evaluating an RL agent, you should create an test environment and evaluate the agent performance in the callback (cf `EvalCallback`). For simplicity, we will be using the training reward as a proxy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDI3lKTiiKP9"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzMHj7r3h78m"
      },
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq, log_dir, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
        "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(\"Saving new best model at {} timesteps\".format(x[-1]))\n",
        "                    print(\"Saving new best model to {}.zip\".format(self.save_path))\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TuYLBEaRUn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9500ca7c-c123-4c6b-bb30-d0b77e019746"
      },
      "source": [
        "# Create log dir\n",
        "log_dir = \"/tmp/gym/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = make_vec_env('CartPole-v1', n_envs=1, monitor_dir=log_dir)\n",
        "# it is equivalent to:\n",
        "# env = gym.make('CartPole-v1')\n",
        "# env = Monitor(env, log_dir)\n",
        "# env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Create Callback\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=20, log_dir=log_dir, verbose=1)\n",
        "\n",
        "model = A2C('MlpPolicy', env, verbose=0)\n",
        "model.learn(total_timesteps=10000, callback=callback)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num timesteps: 20\n",
            "Best mean reward: -inf - Last mean reward per episode: 19.00\n",
            "Saving new best model at 19 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 40\n",
            "Best mean reward: 19.00 - Last mean reward per episode: 14.50\n",
            "Num timesteps: 60\n",
            "Best mean reward: 19.00 - Last mean reward per episode: 14.25\n",
            "Num timesteps: 80\n",
            "Best mean reward: 19.00 - Last mean reward per episode: 15.80\n",
            "Num timesteps: 100\n",
            "Best mean reward: 19.00 - Last mean reward per episode: 16.17\n",
            "Num timesteps: 120\n",
            "Best mean reward: 19.00 - Last mean reward per episode: 15.71\n",
            "Num timesteps: 140\n",
            "Best mean reward: 19.00 - Last mean reward per episode: 15.71\n",
            "Num timesteps: 160\n",
            "Best mean reward: 19.00 - Last mean reward per episode: 15.71\n",
            "Num timesteps: 180\n",
            "Best mean reward: 19.00 - Last mean reward per episode: 21.62\n",
            "Saving new best model at 173 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 200\n",
            "Best mean reward: 21.62 - Last mean reward per episode: 21.78\n",
            "Saving new best model at 196 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 220\n",
            "Best mean reward: 21.78 - Last mean reward per episode: 21.78\n",
            "Num timesteps: 240\n",
            "Best mean reward: 21.78 - Last mean reward per episode: 22.60\n",
            "Saving new best model at 226 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 260\n",
            "Best mean reward: 22.60 - Last mean reward per episode: 21.50\n",
            "Num timesteps: 280\n",
            "Best mean reward: 22.60 - Last mean reward per episode: 21.00\n",
            "Num timesteps: 300\n",
            "Best mean reward: 22.60 - Last mean reward per episode: 21.00\n",
            "Num timesteps: 320\n",
            "Best mean reward: 22.60 - Last mean reward per episode: 21.86\n",
            "Num timesteps: 340\n",
            "Best mean reward: 22.60 - Last mean reward per episode: 21.12\n",
            "Num timesteps: 360\n",
            "Best mean reward: 22.60 - Last mean reward per episode: 21.12\n",
            "Num timesteps: 380\n",
            "Best mean reward: 22.60 - Last mean reward per episode: 21.82\n",
            "Num timesteps: 400\n",
            "Best mean reward: 22.60 - Last mean reward per episode: 21.67\n",
            "Num timesteps: 420\n",
            "Best mean reward: 22.60 - Last mean reward per episode: 21.67\n",
            "Num timesteps: 440\n",
            "Best mean reward: 22.60 - Last mean reward per episode: 22.26\n",
            "Num timesteps: 460\n",
            "Best mean reward: 22.60 - Last mean reward per episode: 22.40\n",
            "Num timesteps: 480\n",
            "Best mean reward: 22.60 - Last mean reward per episode: 22.62\n",
            "Saving new best model at 475 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 500\n",
            "Best mean reward: 22.62 - Last mean reward per episode: 22.14\n",
            "Num timesteps: 520\n",
            "Best mean reward: 22.62 - Last mean reward per episode: 22.35\n",
            "Num timesteps: 540\n",
            "Best mean reward: 22.62 - Last mean reward per episode: 22.00\n",
            "Num timesteps: 560\n",
            "Best mean reward: 22.62 - Last mean reward per episode: 21.46\n",
            "Num timesteps: 580\n",
            "Best mean reward: 22.62 - Last mean reward per episode: 21.46\n",
            "Num timesteps: 600\n",
            "Best mean reward: 22.62 - Last mean reward per episode: 21.46\n",
            "Num timesteps: 620\n",
            "Best mean reward: 22.62 - Last mean reward per episode: 22.33\n",
            "Num timesteps: 640\n",
            "Best mean reward: 22.62 - Last mean reward per episode: 22.64\n",
            "Saving new best model at 634 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 660\n",
            "Best mean reward: 22.64 - Last mean reward per episode: 22.38\n",
            "Num timesteps: 680\n",
            "Best mean reward: 22.64 - Last mean reward per episode: 22.38\n",
            "Num timesteps: 700\n",
            "Best mean reward: 22.64 - Last mean reward per episode: 23.20\n",
            "Saving new best model at 696 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 720\n",
            "Best mean reward: 23.20 - Last mean reward per episode: 23.10\n",
            "Num timesteps: 740\n",
            "Best mean reward: 23.20 - Last mean reward per episode: 22.75\n",
            "Num timesteps: 760\n",
            "Best mean reward: 23.20 - Last mean reward per episode: 22.75\n",
            "Num timesteps: 780\n",
            "Best mean reward: 23.20 - Last mean reward per episode: 23.18\n",
            "Num timesteps: 800\n",
            "Best mean reward: 23.20 - Last mean reward per episode: 23.32\n",
            "Saving new best model at 793 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 820\n",
            "Best mean reward: 23.32 - Last mean reward per episode: 23.32\n",
            "Num timesteps: 840\n",
            "Best mean reward: 23.32 - Last mean reward per episode: 23.32\n",
            "Num timesteps: 860\n",
            "Best mean reward: 23.32 - Last mean reward per episode: 23.32\n",
            "Num timesteps: 880\n",
            "Best mean reward: 23.32 - Last mean reward per episode: 25.00\n",
            "Saving new best model at 875 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 900\n",
            "Best mean reward: 25.00 - Last mean reward per episode: 25.00\n",
            "Num timesteps: 920\n",
            "Best mean reward: 25.00 - Last mean reward per episode: 25.00\n",
            "Num timesteps: 940\n",
            "Best mean reward: 25.00 - Last mean reward per episode: 25.19\n",
            "Saving new best model at 932 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 960\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 24.89\n",
            "Num timesteps: 980\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 24.92\n",
            "Num timesteps: 1000\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 24.39\n",
            "Num timesteps: 1020\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 24.29\n",
            "Num timesteps: 1040\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 24.29\n",
            "Num timesteps: 1060\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 24.49\n",
            "Num timesteps: 1080\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 24.23\n",
            "Num timesteps: 1100\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 23.80\n",
            "Num timesteps: 1120\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 23.80\n",
            "Num timesteps: 1140\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 23.80\n",
            "Num timesteps: 1160\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 24.30\n",
            "Num timesteps: 1180\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 24.30\n",
            "Num timesteps: 1200\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 24.69\n",
            "Num timesteps: 1220\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 24.69\n",
            "Num timesteps: 1240\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 24.69\n",
            "Num timesteps: 1260\n",
            "Best mean reward: 25.19 - Last mean reward per episode: 25.45\n",
            "Saving new best model at 1247 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 1280\n",
            "Best mean reward: 25.45 - Last mean reward per episode: 25.38\n",
            "Num timesteps: 1300\n",
            "Best mean reward: 25.45 - Last mean reward per episode: 25.24\n",
            "Num timesteps: 1320\n",
            "Best mean reward: 25.45 - Last mean reward per episode: 25.24\n",
            "Num timesteps: 1340\n",
            "Best mean reward: 25.45 - Last mean reward per episode: 25.54\n",
            "Saving new best model at 1328 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 1360\n",
            "Best mean reward: 25.54 - Last mean reward per episode: 25.49\n",
            "Num timesteps: 1380\n",
            "Best mean reward: 25.54 - Last mean reward per episode: 25.49\n",
            "Num timesteps: 1400\n",
            "Best mean reward: 25.54 - Last mean reward per episode: 25.83\n",
            "Saving new best model at 1395 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 1420\n",
            "Best mean reward: 25.83 - Last mean reward per episode: 25.78\n",
            "Num timesteps: 1440\n",
            "Best mean reward: 25.83 - Last mean reward per episode: 25.78\n",
            "Num timesteps: 1460\n",
            "Best mean reward: 25.83 - Last mean reward per episode: 25.78\n",
            "Num timesteps: 1480\n",
            "Best mean reward: 25.83 - Last mean reward per episode: 26.12\n",
            "Saving new best model at 1463 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 1500\n",
            "Best mean reward: 26.12 - Last mean reward per episode: 26.16\n",
            "Saving new best model at 1491 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 1520\n",
            "Best mean reward: 26.16 - Last mean reward per episode: 26.16\n",
            "Num timesteps: 1540\n",
            "Best mean reward: 26.16 - Last mean reward per episode: 26.16\n",
            "Num timesteps: 1560\n",
            "Best mean reward: 26.16 - Last mean reward per episode: 26.88\n",
            "Saving new best model at 1559 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 1580\n",
            "Best mean reward: 26.88 - Last mean reward per episode: 26.88\n",
            "Num timesteps: 1600\n",
            "Best mean reward: 26.88 - Last mean reward per episode: 26.88\n",
            "Num timesteps: 1620\n",
            "Best mean reward: 26.88 - Last mean reward per episode: 27.39\n",
            "Saving new best model at 1616 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 1640\n",
            "Best mean reward: 27.39 - Last mean reward per episode: 27.39\n",
            "Num timesteps: 1660\n",
            "Best mean reward: 27.39 - Last mean reward per episode: 27.57\n",
            "Saving new best model at 1654 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 1680\n",
            "Best mean reward: 27.57 - Last mean reward per episode: 27.51\n",
            "Num timesteps: 1700\n",
            "Best mean reward: 27.57 - Last mean reward per episode: 27.51\n",
            "Num timesteps: 1720\n",
            "Best mean reward: 27.57 - Last mean reward per episode: 27.55\n",
            "Num timesteps: 1740\n",
            "Best mean reward: 27.57 - Last mean reward per episode: 27.62\n",
            "Saving new best model at 1740 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 1760\n",
            "Best mean reward: 27.62 - Last mean reward per episode: 27.62\n",
            "Num timesteps: 1780\n",
            "Best mean reward: 27.62 - Last mean reward per episode: 27.56\n",
            "Num timesteps: 1800\n",
            "Best mean reward: 27.62 - Last mean reward per episode: 27.56\n",
            "Num timesteps: 1820\n",
            "Best mean reward: 27.62 - Last mean reward per episode: 27.56\n",
            "Num timesteps: 1840\n",
            "Best mean reward: 27.62 - Last mean reward per episode: 28.02\n",
            "Saving new best model at 1821 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 1860\n",
            "Best mean reward: 28.02 - Last mean reward per episode: 28.02\n",
            "Num timesteps: 1880\n",
            "Best mean reward: 28.02 - Last mean reward per episode: 28.44\n",
            "Saving new best model at 1877 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 1900\n",
            "Best mean reward: 28.44 - Last mean reward per episode: 28.44\n",
            "Num timesteps: 1920\n",
            "Best mean reward: 28.44 - Last mean reward per episode: 28.60\n",
            "Saving new best model at 1916 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 1940\n",
            "Best mean reward: 28.60 - Last mean reward per episode: 28.60\n",
            "Num timesteps: 1960\n",
            "Best mean reward: 28.60 - Last mean reward per episode: 28.60\n",
            "Num timesteps: 1980\n",
            "Best mean reward: 28.60 - Last mean reward per episode: 28.60\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 28.60 - Last mean reward per episode: 28.60\n",
            "Num timesteps: 2020\n",
            "Best mean reward: 28.60 - Last mean reward per episode: 29.63\n",
            "Saving new best model at 2015 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 2040\n",
            "Best mean reward: 29.63 - Last mean reward per episode: 29.63\n",
            "Num timesteps: 2060\n",
            "Best mean reward: 29.63 - Last mean reward per episode: 29.63\n",
            "Num timesteps: 2080\n",
            "Best mean reward: 29.63 - Last mean reward per episode: 30.00\n",
            "Saving new best model at 2070 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 2100\n",
            "Best mean reward: 30.00 - Last mean reward per episode: 30.00\n",
            "Num timesteps: 2120\n",
            "Best mean reward: 30.00 - Last mean reward per episode: 30.19\n",
            "Saving new best model at 2113 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 2140\n",
            "Best mean reward: 30.19 - Last mean reward per episode: 30.19\n",
            "Num timesteps: 2160\n",
            "Best mean reward: 30.19 - Last mean reward per episode: 30.19\n",
            "Num timesteps: 2180\n",
            "Best mean reward: 30.19 - Last mean reward per episode: 30.19\n",
            "Num timesteps: 2200\n",
            "Best mean reward: 30.19 - Last mean reward per episode: 30.76\n",
            "Saving new best model at 2184 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 2220\n",
            "Best mean reward: 30.76 - Last mean reward per episode: 30.76\n",
            "Num timesteps: 2240\n",
            "Best mean reward: 30.76 - Last mean reward per episode: 30.76\n",
            "Num timesteps: 2260\n",
            "Best mean reward: 30.76 - Last mean reward per episode: 31.22\n",
            "Saving new best model at 2248 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 2280\n",
            "Best mean reward: 31.22 - Last mean reward per episode: 30.76\n",
            "Num timesteps: 2300\n",
            "Best mean reward: 31.22 - Last mean reward per episode: 30.76\n",
            "Num timesteps: 2320\n",
            "Best mean reward: 31.22 - Last mean reward per episode: 30.71\n",
            "Num timesteps: 2340\n",
            "Best mean reward: 31.22 - Last mean reward per episode: 30.71\n",
            "Num timesteps: 2360\n",
            "Best mean reward: 31.22 - Last mean reward per episode: 30.71\n",
            "Num timesteps: 2380\n",
            "Best mean reward: 31.22 - Last mean reward per episode: 31.32\n",
            "Saving new best model at 2380 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 2400\n",
            "Best mean reward: 31.32 - Last mean reward per episode: 31.32\n",
            "Num timesteps: 2420\n",
            "Best mean reward: 31.32 - Last mean reward per episode: 31.32\n",
            "Num timesteps: 2440\n",
            "Best mean reward: 31.32 - Last mean reward per episode: 31.65\n",
            "Saving new best model at 2437 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 2460\n",
            "Best mean reward: 31.65 - Last mean reward per episode: 31.50\n",
            "Num timesteps: 2480\n",
            "Best mean reward: 31.65 - Last mean reward per episode: 31.50\n",
            "Num timesteps: 2500\n",
            "Best mean reward: 31.65 - Last mean reward per episode: 31.50\n",
            "Num timesteps: 2520\n",
            "Best mean reward: 31.65 - Last mean reward per episode: 31.50\n",
            "Num timesteps: 2540\n",
            "Best mean reward: 31.65 - Last mean reward per episode: 31.50\n",
            "Num timesteps: 2560\n",
            "Best mean reward: 31.65 - Last mean reward per episode: 31.50\n",
            "Num timesteps: 2580\n",
            "Best mean reward: 31.65 - Last mean reward per episode: 32.53\n",
            "Saving new best model at 2570 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 2600\n",
            "Best mean reward: 32.53 - Last mean reward per episode: 32.53\n",
            "Num timesteps: 2620\n",
            "Best mean reward: 32.53 - Last mean reward per episode: 32.53\n",
            "Num timesteps: 2640\n",
            "Best mean reward: 32.53 - Last mean reward per episode: 32.53\n",
            "Num timesteps: 2660\n",
            "Best mean reward: 32.53 - Last mean reward per episode: 32.53\n",
            "Num timesteps: 2680\n",
            "Best mean reward: 32.53 - Last mean reward per episode: 33.38\n",
            "Saving new best model at 2670 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 2700\n",
            "Best mean reward: 33.38 - Last mean reward per episode: 33.38\n",
            "Num timesteps: 2720\n",
            "Best mean reward: 33.38 - Last mean reward per episode: 33.38\n",
            "Num timesteps: 2740\n",
            "Best mean reward: 33.38 - Last mean reward per episode: 33.70\n",
            "Saving new best model at 2730 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 2760\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 33.62\n",
            "Num timesteps: 2780\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 33.62\n",
            "Num timesteps: 2800\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 33.60\n",
            "Num timesteps: 2820\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 33.36\n",
            "Num timesteps: 2840\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 33.36\n",
            "Num timesteps: 2860\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 33.59\n",
            "Num timesteps: 2880\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 33.59\n",
            "Num timesteps: 2900\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 33.59\n",
            "Num timesteps: 2920\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 33.59\n",
            "Num timesteps: 2940\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 33.59\n",
            "Num timesteps: 2960\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 33.59\n",
            "Num timesteps: 2980\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 33.59\n",
            "Num timesteps: 3000\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 33.59\n",
            "Num timesteps: 3020\n",
            "Best mean reward: 33.70 - Last mean reward per episode: 34.91\n",
            "Saving new best model at 3002 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 3040\n",
            "Best mean reward: 34.91 - Last mean reward per episode: 34.76\n",
            "Num timesteps: 3060\n",
            "Best mean reward: 34.91 - Last mean reward per episode: 34.76\n",
            "Num timesteps: 3080\n",
            "Best mean reward: 34.91 - Last mean reward per episode: 34.76\n",
            "Num timesteps: 3100\n",
            "Best mean reward: 34.91 - Last mean reward per episode: 34.76\n",
            "Num timesteps: 3120\n",
            "Best mean reward: 34.91 - Last mean reward per episode: 34.76\n",
            "Num timesteps: 3140\n",
            "Best mean reward: 34.91 - Last mean reward per episode: 34.76\n",
            "Num timesteps: 3160\n",
            "Best mean reward: 34.91 - Last mean reward per episode: 34.76\n",
            "Num timesteps: 3180\n",
            "Best mean reward: 34.91 - Last mean reward per episode: 36.14\n",
            "Saving new best model at 3180 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 3200\n",
            "Best mean reward: 36.14 - Last mean reward per episode: 36.14\n",
            "Num timesteps: 3220\n",
            "Best mean reward: 36.14 - Last mean reward per episode: 36.14\n",
            "Num timesteps: 3240\n",
            "Best mean reward: 36.14 - Last mean reward per episode: 36.24\n",
            "Saving new best model at 3225 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 3260\n",
            "Best mean reward: 36.24 - Last mean reward per episode: 36.24\n",
            "Num timesteps: 3280\n",
            "Best mean reward: 36.24 - Last mean reward per episode: 36.24\n",
            "Num timesteps: 3300\n",
            "Best mean reward: 36.24 - Last mean reward per episode: 36.24\n",
            "Num timesteps: 3320\n",
            "Best mean reward: 36.24 - Last mean reward per episode: 36.24\n",
            "Num timesteps: 3340\n",
            "Best mean reward: 36.24 - Last mean reward per episode: 36.24\n",
            "Num timesteps: 3360\n",
            "Best mean reward: 36.24 - Last mean reward per episode: 36.24\n",
            "Num timesteps: 3380\n",
            "Best mean reward: 36.24 - Last mean reward per episode: 37.39\n",
            "Saving new best model at 3365 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 3400\n",
            "Best mean reward: 37.39 - Last mean reward per episode: 37.39\n",
            "Num timesteps: 3420\n",
            "Best mean reward: 37.39 - Last mean reward per episode: 37.39\n",
            "Num timesteps: 3440\n",
            "Best mean reward: 37.39 - Last mean reward per episode: 37.39\n",
            "Num timesteps: 3460\n",
            "Best mean reward: 37.39 - Last mean reward per episode: 37.39\n",
            "Num timesteps: 3480\n",
            "Best mean reward: 37.39 - Last mean reward per episode: 37.39\n",
            "Num timesteps: 3500\n",
            "Best mean reward: 37.39 - Last mean reward per episode: 37.39\n",
            "Num timesteps: 3520\n",
            "Best mean reward: 37.39 - Last mean reward per episode: 38.60\n",
            "Saving new best model at 3513 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 3540\n",
            "Best mean reward: 38.60 - Last mean reward per episode: 38.60\n",
            "Num timesteps: 3560\n",
            "Best mean reward: 38.60 - Last mean reward per episode: 38.60\n",
            "Num timesteps: 3580\n",
            "Best mean reward: 38.60 - Last mean reward per episode: 38.60\n",
            "Num timesteps: 3600\n",
            "Best mean reward: 38.60 - Last mean reward per episode: 38.60\n",
            "Num timesteps: 3620\n",
            "Best mean reward: 38.60 - Last mean reward per episode: 39.22\n",
            "Saving new best model at 3608 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 3640\n",
            "Best mean reward: 39.22 - Last mean reward per episode: 39.00\n",
            "Num timesteps: 3660\n",
            "Best mean reward: 39.22 - Last mean reward per episode: 39.00\n",
            "Num timesteps: 3680\n",
            "Best mean reward: 39.22 - Last mean reward per episode: 39.00\n",
            "Num timesteps: 3700\n",
            "Best mean reward: 39.22 - Last mean reward per episode: 39.30\n",
            "Saving new best model at 3694 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 3720\n",
            "Best mean reward: 39.30 - Last mean reward per episode: 39.30\n",
            "Num timesteps: 3740\n",
            "Best mean reward: 39.30 - Last mean reward per episode: 39.30\n",
            "Num timesteps: 3760\n",
            "Best mean reward: 39.30 - Last mean reward per episode: 39.30\n",
            "Num timesteps: 3780\n",
            "Best mean reward: 39.30 - Last mean reward per episode: 39.30\n",
            "Num timesteps: 3800\n",
            "Best mean reward: 39.30 - Last mean reward per episode: 39.30\n",
            "Num timesteps: 3820\n",
            "Best mean reward: 39.30 - Last mean reward per episode: 39.30\n",
            "Num timesteps: 3840\n",
            "Best mean reward: 39.30 - Last mean reward per episode: 39.30\n",
            "Num timesteps: 3860\n",
            "Best mean reward: 39.30 - Last mean reward per episode: 39.30\n",
            "Num timesteps: 3880\n",
            "Best mean reward: 39.30 - Last mean reward per episode: 40.73\n",
            "Saving new best model at 3869 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 3900\n",
            "Best mean reward: 40.73 - Last mean reward per episode: 40.73\n",
            "Num timesteps: 3920\n",
            "Best mean reward: 40.73 - Last mean reward per episode: 40.73\n",
            "Num timesteps: 3940\n",
            "Best mean reward: 40.73 - Last mean reward per episode: 40.73\n",
            "Num timesteps: 3960\n",
            "Best mean reward: 40.73 - Last mean reward per episode: 40.73\n",
            "Num timesteps: 3980\n",
            "Best mean reward: 40.73 - Last mean reward per episode: 40.73\n",
            "Num timesteps: 4000\n",
            "Best mean reward: 40.73 - Last mean reward per episode: 41.64\n",
            "Saving new best model at 3997 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 4020\n",
            "Best mean reward: 41.64 - Last mean reward per episode: 41.64\n",
            "Num timesteps: 4040\n",
            "Best mean reward: 41.64 - Last mean reward per episode: 41.64\n",
            "Num timesteps: 4060\n",
            "Best mean reward: 41.64 - Last mean reward per episode: 41.64\n",
            "Num timesteps: 4080\n",
            "Best mean reward: 41.64 - Last mean reward per episode: 41.64\n",
            "Num timesteps: 4100\n",
            "Best mean reward: 41.64 - Last mean reward per episode: 41.64\n",
            "Num timesteps: 4120\n",
            "Best mean reward: 41.64 - Last mean reward per episode: 41.64\n",
            "Num timesteps: 4140\n",
            "Best mean reward: 41.64 - Last mean reward per episode: 41.64\n",
            "Num timesteps: 4160\n",
            "Best mean reward: 41.64 - Last mean reward per episode: 41.64\n",
            "Num timesteps: 4180\n",
            "Best mean reward: 41.64 - Last mean reward per episode: 41.64\n",
            "Num timesteps: 4200\n",
            "Best mean reward: 41.64 - Last mean reward per episode: 41.64\n",
            "Num timesteps: 4220\n",
            "Best mean reward: 41.64 - Last mean reward per episode: 41.64\n",
            "Num timesteps: 4240\n",
            "Best mean reward: 41.64 - Last mean reward per episode: 41.64\n",
            "Num timesteps: 4260\n",
            "Best mean reward: 41.64 - Last mean reward per episode: 43.82\n",
            "Saving new best model at 4251 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 4280\n",
            "Best mean reward: 43.82 - Last mean reward per episode: 43.82\n",
            "Num timesteps: 4300\n",
            "Best mean reward: 43.82 - Last mean reward per episode: 43.82\n",
            "Num timesteps: 4320\n",
            "Best mean reward: 43.82 - Last mean reward per episode: 43.82\n",
            "Num timesteps: 4340\n",
            "Best mean reward: 43.82 - Last mean reward per episode: 44.28\n",
            "Saving new best model at 4339 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 4360\n",
            "Best mean reward: 44.28 - Last mean reward per episode: 44.28\n",
            "Num timesteps: 4380\n",
            "Best mean reward: 44.28 - Last mean reward per episode: 44.28\n",
            "Num timesteps: 4400\n",
            "Best mean reward: 44.28 - Last mean reward per episode: 44.28\n",
            "Num timesteps: 4420\n",
            "Best mean reward: 44.28 - Last mean reward per episode: 44.28\n",
            "Num timesteps: 4440\n",
            "Best mean reward: 44.28 - Last mean reward per episode: 44.28\n",
            "Num timesteps: 4460\n",
            "Best mean reward: 44.28 - Last mean reward per episode: 44.28\n",
            "Num timesteps: 4480\n",
            "Best mean reward: 44.28 - Last mean reward per episode: 44.28\n",
            "Num timesteps: 4500\n",
            "Best mean reward: 44.28 - Last mean reward per episode: 44.28\n",
            "Num timesteps: 4520\n",
            "Best mean reward: 44.28 - Last mean reward per episode: 44.28\n",
            "Num timesteps: 4540\n",
            "Best mean reward: 44.28 - Last mean reward per episode: 45.79\n",
            "Saving new best model at 4533 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 4560\n",
            "Best mean reward: 45.79 - Last mean reward per episode: 45.79\n",
            "Num timesteps: 4580\n",
            "Best mean reward: 45.79 - Last mean reward per episode: 45.79\n",
            "Num timesteps: 4600\n",
            "Best mean reward: 45.79 - Last mean reward per episode: 45.79\n",
            "Num timesteps: 4620\n",
            "Best mean reward: 45.79 - Last mean reward per episode: 46.11\n",
            "Saving new best model at 4611 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 4640\n",
            "Best mean reward: 46.11 - Last mean reward per episode: 46.11\n",
            "Num timesteps: 4660\n",
            "Best mean reward: 46.11 - Last mean reward per episode: 46.11\n",
            "Num timesteps: 4680\n",
            "Best mean reward: 46.11 - Last mean reward per episode: 46.11\n",
            "Num timesteps: 4700\n",
            "Best mean reward: 46.11 - Last mean reward per episode: 46.11\n",
            "Num timesteps: 4720\n",
            "Best mean reward: 46.11 - Last mean reward per episode: 46.97\n",
            "Saving new best model at 4716 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 4740\n",
            "Best mean reward: 46.97 - Last mean reward per episode: 46.97\n",
            "Num timesteps: 4760\n",
            "Best mean reward: 46.97 - Last mean reward per episode: 46.97\n",
            "Num timesteps: 4780\n",
            "Best mean reward: 46.97 - Last mean reward per episode: 46.97\n",
            "Num timesteps: 4800\n",
            "Best mean reward: 46.97 - Last mean reward per episode: 46.97\n",
            "Num timesteps: 4820\n",
            "Best mean reward: 46.97 - Last mean reward per episode: 47.79\n",
            "Saving new best model at 4808 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 4840\n",
            "Best mean reward: 47.79 - Last mean reward per episode: 47.79\n",
            "Num timesteps: 4860\n",
            "Best mean reward: 47.79 - Last mean reward per episode: 47.79\n",
            "Num timesteps: 4880\n",
            "Best mean reward: 47.79 - Last mean reward per episode: 48.30\n",
            "Saving new best model at 4874 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 4900\n",
            "Best mean reward: 48.30 - Last mean reward per episode: 48.30\n",
            "Num timesteps: 4920\n",
            "Best mean reward: 48.30 - Last mean reward per episode: 48.30\n",
            "Num timesteps: 4940\n",
            "Best mean reward: 48.30 - Last mean reward per episode: 48.30\n",
            "Num timesteps: 4960\n",
            "Best mean reward: 48.30 - Last mean reward per episode: 48.94\n",
            "Saving new best model at 4951 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 4980\n",
            "Best mean reward: 48.94 - Last mean reward per episode: 48.94\n",
            "Num timesteps: 5000\n",
            "Best mean reward: 48.94 - Last mean reward per episode: 48.94\n",
            "Num timesteps: 5020\n",
            "Best mean reward: 48.94 - Last mean reward per episode: 49.31\n",
            "Saving new best model at 5010 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 5040\n",
            "Best mean reward: 49.31 - Last mean reward per episode: 49.31\n",
            "Num timesteps: 5060\n",
            "Best mean reward: 49.31 - Last mean reward per episode: 49.31\n",
            "Num timesteps: 5080\n",
            "Best mean reward: 49.31 - Last mean reward per episode: 49.31\n",
            "Num timesteps: 5100\n",
            "Best mean reward: 49.31 - Last mean reward per episode: 49.85\n",
            "Saving new best model at 5082 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 5120\n",
            "Best mean reward: 49.85 - Last mean reward per episode: 49.85\n",
            "Num timesteps: 5140\n",
            "Best mean reward: 49.85 - Last mean reward per episode: 49.85\n",
            "Num timesteps: 5160\n",
            "Best mean reward: 49.85 - Last mean reward per episode: 50.39\n",
            "Saving new best model at 5149 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 5180\n",
            "Best mean reward: 50.39 - Last mean reward per episode: 49.92\n",
            "Num timesteps: 5200\n",
            "Best mean reward: 50.39 - Last mean reward per episode: 49.92\n",
            "Num timesteps: 5220\n",
            "Best mean reward: 50.39 - Last mean reward per episode: 49.92\n",
            "Num timesteps: 5240\n",
            "Best mean reward: 50.39 - Last mean reward per episode: 49.92\n",
            "Num timesteps: 5260\n",
            "Best mean reward: 50.39 - Last mean reward per episode: 50.62\n",
            "Saving new best model at 5258 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 5280\n",
            "Best mean reward: 50.62 - Last mean reward per episode: 50.62\n",
            "Num timesteps: 5300\n",
            "Best mean reward: 50.62 - Last mean reward per episode: 50.61\n",
            "Num timesteps: 5320\n",
            "Best mean reward: 50.62 - Last mean reward per episode: 50.61\n",
            "Num timesteps: 5340\n",
            "Best mean reward: 50.62 - Last mean reward per episode: 50.61\n",
            "Num timesteps: 5360\n",
            "Best mean reward: 50.62 - Last mean reward per episode: 51.15\n",
            "Saving new best model at 5358 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 5380\n",
            "Best mean reward: 51.15 - Last mean reward per episode: 51.15\n",
            "Num timesteps: 5400\n",
            "Best mean reward: 51.15 - Last mean reward per episode: 51.28\n",
            "Saving new best model at 5386 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 5420\n",
            "Best mean reward: 51.28 - Last mean reward per episode: 51.28\n",
            "Num timesteps: 5440\n",
            "Best mean reward: 51.28 - Last mean reward per episode: 51.28\n",
            "Num timesteps: 5460\n",
            "Best mean reward: 51.28 - Last mean reward per episode: 51.28\n",
            "Num timesteps: 5480\n",
            "Best mean reward: 51.28 - Last mean reward per episode: 52.00\n",
            "Saving new best model at 5473 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 5500\n",
            "Best mean reward: 52.00 - Last mean reward per episode: 52.00\n",
            "Num timesteps: 5520\n",
            "Best mean reward: 52.00 - Last mean reward per episode: 52.00\n",
            "Num timesteps: 5540\n",
            "Best mean reward: 52.00 - Last mean reward per episode: 52.00\n",
            "Num timesteps: 5560\n",
            "Best mean reward: 52.00 - Last mean reward per episode: 52.35\n",
            "Saving new best model at 5541 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 5580\n",
            "Best mean reward: 52.35 - Last mean reward per episode: 52.35\n",
            "Num timesteps: 5600\n",
            "Best mean reward: 52.35 - Last mean reward per episode: 52.35\n",
            "Num timesteps: 5620\n",
            "Best mean reward: 52.35 - Last mean reward per episode: 52.35\n",
            "Num timesteps: 5640\n",
            "Best mean reward: 52.35 - Last mean reward per episode: 53.13\n",
            "Saving new best model at 5640 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 5660\n",
            "Best mean reward: 53.13 - Last mean reward per episode: 53.20\n",
            "Saving new best model at 5658 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 5680\n",
            "Best mean reward: 53.20 - Last mean reward per episode: 53.20\n",
            "Num timesteps: 5700\n",
            "Best mean reward: 53.20 - Last mean reward per episode: 53.20\n",
            "Num timesteps: 5720\n",
            "Best mean reward: 53.20 - Last mean reward per episode: 53.20\n",
            "Num timesteps: 5740\n",
            "Best mean reward: 53.20 - Last mean reward per episode: 53.20\n",
            "Num timesteps: 5760\n",
            "Best mean reward: 53.20 - Last mean reward per episode: 53.20\n",
            "Num timesteps: 5780\n",
            "Best mean reward: 53.20 - Last mean reward per episode: 53.20\n",
            "Num timesteps: 5800\n",
            "Best mean reward: 53.20 - Last mean reward per episode: 53.20\n",
            "Num timesteps: 5820\n",
            "Best mean reward: 53.20 - Last mean reward per episode: 53.20\n",
            "Num timesteps: 5840\n",
            "Best mean reward: 53.20 - Last mean reward per episode: 53.20\n",
            "Num timesteps: 5860\n",
            "Best mean reward: 53.20 - Last mean reward per episode: 54.75\n",
            "Saving new best model at 5846 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 5880\n",
            "Best mean reward: 54.75 - Last mean reward per episode: 54.75\n",
            "Num timesteps: 5900\n",
            "Best mean reward: 54.75 - Last mean reward per episode: 54.75\n",
            "Num timesteps: 5920\n",
            "Best mean reward: 54.75 - Last mean reward per episode: 54.75\n",
            "Num timesteps: 5940\n",
            "Best mean reward: 54.75 - Last mean reward per episode: 54.75\n",
            "Num timesteps: 5960\n",
            "Best mean reward: 54.75 - Last mean reward per episode: 54.75\n",
            "Num timesteps: 5980\n",
            "Best mean reward: 54.75 - Last mean reward per episode: 54.75\n",
            "Num timesteps: 6000\n",
            "Best mean reward: 54.75 - Last mean reward per episode: 54.75\n",
            "Num timesteps: 6020\n",
            "Best mean reward: 54.75 - Last mean reward per episode: 54.75\n",
            "Num timesteps: 6040\n",
            "Best mean reward: 54.75 - Last mean reward per episode: 54.75\n",
            "Num timesteps: 6060\n",
            "Best mean reward: 54.75 - Last mean reward per episode: 54.75\n",
            "Num timesteps: 6080\n",
            "Best mean reward: 54.75 - Last mean reward per episode: 56.81\n",
            "Saving new best model at 6071 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 6100\n",
            "Best mean reward: 56.81 - Last mean reward per episode: 56.75\n",
            "Num timesteps: 6120\n",
            "Best mean reward: 56.81 - Last mean reward per episode: 56.75\n",
            "Num timesteps: 6140\n",
            "Best mean reward: 56.81 - Last mean reward per episode: 56.75\n",
            "Num timesteps: 6160\n",
            "Best mean reward: 56.81 - Last mean reward per episode: 56.75\n",
            "Num timesteps: 6180\n",
            "Best mean reward: 56.81 - Last mean reward per episode: 56.75\n",
            "Num timesteps: 6200\n",
            "Best mean reward: 56.81 - Last mean reward per episode: 56.75\n",
            "Num timesteps: 6220\n",
            "Best mean reward: 56.81 - Last mean reward per episode: 56.75\n",
            "Num timesteps: 6240\n",
            "Best mean reward: 56.81 - Last mean reward per episode: 56.75\n",
            "Num timesteps: 6260\n",
            "Best mean reward: 56.81 - Last mean reward per episode: 57.99\n",
            "Saving new best model at 6247 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 6280\n",
            "Best mean reward: 57.99 - Last mean reward per episode: 57.99\n",
            "Num timesteps: 6300\n",
            "Best mean reward: 57.99 - Last mean reward per episode: 58.24\n",
            "Saving new best model at 6299 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 6320\n",
            "Best mean reward: 58.24 - Last mean reward per episode: 58.24\n",
            "Num timesteps: 6340\n",
            "Best mean reward: 58.24 - Last mean reward per episode: 58.24\n",
            "Num timesteps: 6360\n",
            "Best mean reward: 58.24 - Last mean reward per episode: 58.24\n",
            "Num timesteps: 6380\n",
            "Best mean reward: 58.24 - Last mean reward per episode: 58.24\n",
            "Num timesteps: 6400\n",
            "Best mean reward: 58.24 - Last mean reward per episode: 59.02\n",
            "Saving new best model at 6389 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 6420\n",
            "Best mean reward: 59.02 - Last mean reward per episode: 59.02\n",
            "Num timesteps: 6440\n",
            "Best mean reward: 59.02 - Last mean reward per episode: 59.02\n",
            "Num timesteps: 6460\n",
            "Best mean reward: 59.02 - Last mean reward per episode: 59.02\n",
            "Num timesteps: 6480\n",
            "Best mean reward: 59.02 - Last mean reward per episode: 59.02\n",
            "Num timesteps: 6500\n",
            "Best mean reward: 59.02 - Last mean reward per episode: 59.85\n",
            "Saving new best model at 6499 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 6520\n",
            "Best mean reward: 59.85 - Last mean reward per episode: 59.85\n",
            "Num timesteps: 6540\n",
            "Best mean reward: 59.85 - Last mean reward per episode: 59.85\n",
            "Num timesteps: 6560\n",
            "Best mean reward: 59.85 - Last mean reward per episode: 59.85\n",
            "Num timesteps: 6580\n",
            "Best mean reward: 59.85 - Last mean reward per episode: 59.85\n",
            "Num timesteps: 6600\n",
            "Best mean reward: 59.85 - Last mean reward per episode: 59.85\n",
            "Num timesteps: 6620\n",
            "Best mean reward: 59.85 - Last mean reward per episode: 59.85\n",
            "Num timesteps: 6640\n",
            "Best mean reward: 59.85 - Last mean reward per episode: 59.85\n",
            "Num timesteps: 6660\n",
            "Best mean reward: 59.85 - Last mean reward per episode: 59.85\n",
            "Num timesteps: 6680\n",
            "Best mean reward: 59.85 - Last mean reward per episode: 59.85\n",
            "Num timesteps: 6700\n",
            "Best mean reward: 59.85 - Last mean reward per episode: 59.85\n",
            "Num timesteps: 6720\n",
            "Best mean reward: 59.85 - Last mean reward per episode: 61.80\n",
            "Saving new best model at 6708 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 6740\n",
            "Best mean reward: 61.80 - Last mean reward per episode: 61.80\n",
            "Num timesteps: 6760\n",
            "Best mean reward: 61.80 - Last mean reward per episode: 61.80\n",
            "Num timesteps: 6780\n",
            "Best mean reward: 61.80 - Last mean reward per episode: 61.80\n",
            "Num timesteps: 6800\n",
            "Best mean reward: 61.80 - Last mean reward per episode: 61.80\n",
            "Num timesteps: 6820\n",
            "Best mean reward: 61.80 - Last mean reward per episode: 61.80\n",
            "Num timesteps: 6840\n",
            "Best mean reward: 61.80 - Last mean reward per episode: 61.80\n",
            "Num timesteps: 6860\n",
            "Best mean reward: 61.80 - Last mean reward per episode: 61.80\n",
            "Num timesteps: 6880\n",
            "Best mean reward: 61.80 - Last mean reward per episode: 61.80\n",
            "Num timesteps: 6900\n",
            "Best mean reward: 61.80 - Last mean reward per episode: 63.50\n",
            "Saving new best model at 6895 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 6920\n",
            "Best mean reward: 63.50 - Last mean reward per episode: 63.50\n",
            "Num timesteps: 6940\n",
            "Best mean reward: 63.50 - Last mean reward per episode: 63.50\n",
            "Num timesteps: 6960\n",
            "Best mean reward: 63.50 - Last mean reward per episode: 63.50\n",
            "Num timesteps: 6980\n",
            "Best mean reward: 63.50 - Last mean reward per episode: 63.50\n",
            "Num timesteps: 7000\n",
            "Best mean reward: 63.50 - Last mean reward per episode: 63.50\n",
            "Num timesteps: 7020\n",
            "Best mean reward: 63.50 - Last mean reward per episode: 63.50\n",
            "Num timesteps: 7040\n",
            "Best mean reward: 63.50 - Last mean reward per episode: 63.50\n",
            "Num timesteps: 7060\n",
            "Best mean reward: 63.50 - Last mean reward per episode: 64.95\n",
            "Saving new best model at 7053 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 7080\n",
            "Best mean reward: 64.95 - Last mean reward per episode: 64.95\n",
            "Num timesteps: 7100\n",
            "Best mean reward: 64.95 - Last mean reward per episode: 64.95\n",
            "Num timesteps: 7120\n",
            "Best mean reward: 64.95 - Last mean reward per episode: 64.95\n",
            "Num timesteps: 7140\n",
            "Best mean reward: 64.95 - Last mean reward per episode: 64.95\n",
            "Num timesteps: 7160\n",
            "Best mean reward: 64.95 - Last mean reward per episode: 64.95\n",
            "Num timesteps: 7180\n",
            "Best mean reward: 64.95 - Last mean reward per episode: 64.95\n",
            "Num timesteps: 7200\n",
            "Best mean reward: 64.95 - Last mean reward per episode: 64.95\n",
            "Num timesteps: 7220\n",
            "Best mean reward: 64.95 - Last mean reward per episode: 64.95\n",
            "Num timesteps: 7240\n",
            "Best mean reward: 64.95 - Last mean reward per episode: 66.28\n",
            "Saving new best model at 7231 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 7260\n",
            "Best mean reward: 66.28 - Last mean reward per episode: 66.28\n",
            "Num timesteps: 7280\n",
            "Best mean reward: 66.28 - Last mean reward per episode: 66.28\n",
            "Num timesteps: 7300\n",
            "Best mean reward: 66.28 - Last mean reward per episode: 66.28\n",
            "Num timesteps: 7320\n",
            "Best mean reward: 66.28 - Last mean reward per episode: 66.28\n",
            "Num timesteps: 7340\n",
            "Best mean reward: 66.28 - Last mean reward per episode: 66.28\n",
            "Num timesteps: 7360\n",
            "Best mean reward: 66.28 - Last mean reward per episode: 66.28\n",
            "Num timesteps: 7380\n",
            "Best mean reward: 66.28 - Last mean reward per episode: 66.28\n",
            "Num timesteps: 7400\n",
            "Best mean reward: 66.28 - Last mean reward per episode: 66.28\n",
            "Num timesteps: 7420\n",
            "Best mean reward: 66.28 - Last mean reward per episode: 66.28\n",
            "Num timesteps: 7440\n",
            "Best mean reward: 66.28 - Last mean reward per episode: 66.28\n",
            "Num timesteps: 7460\n",
            "Best mean reward: 66.28 - Last mean reward per episode: 68.11\n",
            "Saving new best model at 7445 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 7480\n",
            "Best mean reward: 68.11 - Last mean reward per episode: 68.11\n",
            "Num timesteps: 7500\n",
            "Best mean reward: 68.11 - Last mean reward per episode: 68.11\n",
            "Num timesteps: 7520\n",
            "Best mean reward: 68.11 - Last mean reward per episode: 68.65\n",
            "Saving new best model at 7514 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 7540\n",
            "Best mean reward: 68.65 - Last mean reward per episode: 68.65\n",
            "Num timesteps: 7560\n",
            "Best mean reward: 68.65 - Last mean reward per episode: 68.65\n",
            "Num timesteps: 7580\n",
            "Best mean reward: 68.65 - Last mean reward per episode: 68.65\n",
            "Num timesteps: 7600\n",
            "Best mean reward: 68.65 - Last mean reward per episode: 68.65\n",
            "Num timesteps: 7620\n",
            "Best mean reward: 68.65 - Last mean reward per episode: 68.65\n",
            "Num timesteps: 7640\n",
            "Best mean reward: 68.65 - Last mean reward per episode: 68.65\n",
            "Num timesteps: 7660\n",
            "Best mean reward: 68.65 - Last mean reward per episode: 69.46\n",
            "Saving new best model at 7642 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 7680\n",
            "Best mean reward: 69.46 - Last mean reward per episode: 69.46\n",
            "Num timesteps: 7700\n",
            "Best mean reward: 69.46 - Last mean reward per episode: 69.46\n",
            "Num timesteps: 7720\n",
            "Best mean reward: 69.46 - Last mean reward per episode: 69.46\n",
            "Num timesteps: 7740\n",
            "Best mean reward: 69.46 - Last mean reward per episode: 69.46\n",
            "Num timesteps: 7760\n",
            "Best mean reward: 69.46 - Last mean reward per episode: 69.46\n",
            "Num timesteps: 7780\n",
            "Best mean reward: 69.46 - Last mean reward per episode: 69.46\n",
            "Num timesteps: 7800\n",
            "Best mean reward: 69.46 - Last mean reward per episode: 69.46\n",
            "Num timesteps: 7820\n",
            "Best mean reward: 69.46 - Last mean reward per episode: 71.00\n",
            "Saving new best model at 7816 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 7840\n",
            "Best mean reward: 71.00 - Last mean reward per episode: 71.00\n",
            "Num timesteps: 7860\n",
            "Best mean reward: 71.00 - Last mean reward per episode: 71.00\n",
            "Num timesteps: 7880\n",
            "Best mean reward: 71.00 - Last mean reward per episode: 71.00\n",
            "Num timesteps: 7900\n",
            "Best mean reward: 71.00 - Last mean reward per episode: 71.00\n",
            "Num timesteps: 7920\n",
            "Best mean reward: 71.00 - Last mean reward per episode: 71.85\n",
            "Saving new best model at 7913 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 7940\n",
            "Best mean reward: 71.85 - Last mean reward per episode: 71.85\n",
            "Num timesteps: 7960\n",
            "Best mean reward: 71.85 - Last mean reward per episode: 71.85\n",
            "Num timesteps: 7980\n",
            "Best mean reward: 71.85 - Last mean reward per episode: 71.85\n",
            "Num timesteps: 8000\n",
            "Best mean reward: 71.85 - Last mean reward per episode: 71.85\n",
            "Num timesteps: 8020\n",
            "Best mean reward: 71.85 - Last mean reward per episode: 72.49\n",
            "Saving new best model at 8014 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 8040\n",
            "Best mean reward: 72.49 - Last mean reward per episode: 72.49\n",
            "Num timesteps: 8060\n",
            "Best mean reward: 72.49 - Last mean reward per episode: 72.49\n",
            "Num timesteps: 8080\n",
            "Best mean reward: 72.49 - Last mean reward per episode: 72.49\n",
            "Num timesteps: 8100\n",
            "Best mean reward: 72.49 - Last mean reward per episode: 72.49\n",
            "Num timesteps: 8120\n",
            "Best mean reward: 72.49 - Last mean reward per episode: 73.12\n",
            "Saving new best model at 8105 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 8140\n",
            "Best mean reward: 73.12 - Last mean reward per episode: 73.12\n",
            "Num timesteps: 8160\n",
            "Best mean reward: 73.12 - Last mean reward per episode: 73.12\n",
            "Num timesteps: 8180\n",
            "Best mean reward: 73.12 - Last mean reward per episode: 73.12\n",
            "Num timesteps: 8200\n",
            "Best mean reward: 73.12 - Last mean reward per episode: 73.12\n",
            "Num timesteps: 8220\n",
            "Best mean reward: 73.12 - Last mean reward per episode: 73.12\n",
            "Num timesteps: 8240\n",
            "Best mean reward: 73.12 - Last mean reward per episode: 73.12\n",
            "Num timesteps: 8260\n",
            "Best mean reward: 73.12 - Last mean reward per episode: 73.68\n",
            "Saving new best model at 8243 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 8280\n",
            "Best mean reward: 73.68 - Last mean reward per episode: 73.68\n",
            "Num timesteps: 8300\n",
            "Best mean reward: 73.68 - Last mean reward per episode: 73.68\n",
            "Num timesteps: 8320\n",
            "Best mean reward: 73.68 - Last mean reward per episode: 73.68\n",
            "Num timesteps: 8340\n",
            "Best mean reward: 73.68 - Last mean reward per episode: 74.16\n",
            "Saving new best model at 8337 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 8360\n",
            "Best mean reward: 74.16 - Last mean reward per episode: 74.16\n",
            "Num timesteps: 8380\n",
            "Best mean reward: 74.16 - Last mean reward per episode: 74.16\n",
            "Num timesteps: 8400\n",
            "Best mean reward: 74.16 - Last mean reward per episode: 74.16\n",
            "Num timesteps: 8420\n",
            "Best mean reward: 74.16 - Last mean reward per episode: 74.16\n",
            "Num timesteps: 8440\n",
            "Best mean reward: 74.16 - Last mean reward per episode: 74.16\n",
            "Num timesteps: 8460\n",
            "Best mean reward: 74.16 - Last mean reward per episode: 75.16\n",
            "Saving new best model at 8448 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 8480\n",
            "Best mean reward: 75.16 - Last mean reward per episode: 75.16\n",
            "Num timesteps: 8500\n",
            "Best mean reward: 75.16 - Last mean reward per episode: 75.16\n",
            "Num timesteps: 8520\n",
            "Best mean reward: 75.16 - Last mean reward per episode: 75.16\n",
            "Num timesteps: 8540\n",
            "Best mean reward: 75.16 - Last mean reward per episode: 75.16\n",
            "Num timesteps: 8560\n",
            "Best mean reward: 75.16 - Last mean reward per episode: 75.16\n",
            "Num timesteps: 8580\n",
            "Best mean reward: 75.16 - Last mean reward per episode: 75.16\n",
            "Num timesteps: 8600\n",
            "Best mean reward: 75.16 - Last mean reward per episode: 75.16\n",
            "Num timesteps: 8620\n",
            "Best mean reward: 75.16 - Last mean reward per episode: 75.16\n",
            "Num timesteps: 8640\n",
            "Best mean reward: 75.16 - Last mean reward per episode: 75.16\n",
            "Num timesteps: 8660\n",
            "Best mean reward: 75.16 - Last mean reward per episode: 77.09\n",
            "Saving new best model at 8655 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 8680\n",
            "Best mean reward: 77.09 - Last mean reward per episode: 77.09\n",
            "Num timesteps: 8700\n",
            "Best mean reward: 77.09 - Last mean reward per episode: 77.09\n",
            "Num timesteps: 8720\n",
            "Best mean reward: 77.09 - Last mean reward per episode: 77.09\n",
            "Num timesteps: 8740\n",
            "Best mean reward: 77.09 - Last mean reward per episode: 77.09\n",
            "Num timesteps: 8760\n",
            "Best mean reward: 77.09 - Last mean reward per episode: 77.81\n",
            "Saving new best model at 8753 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 8780\n",
            "Best mean reward: 77.81 - Last mean reward per episode: 77.81\n",
            "Num timesteps: 8800\n",
            "Best mean reward: 77.81 - Last mean reward per episode: 77.81\n",
            "Num timesteps: 8820\n",
            "Best mean reward: 77.81 - Last mean reward per episode: 77.81\n",
            "Num timesteps: 8840\n",
            "Best mean reward: 77.81 - Last mean reward per episode: 77.81\n",
            "Num timesteps: 8860\n",
            "Best mean reward: 77.81 - Last mean reward per episode: 78.60\n",
            "Saving new best model at 8849 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 8880\n",
            "Best mean reward: 78.60 - Last mean reward per episode: 78.60\n",
            "Num timesteps: 8900\n",
            "Best mean reward: 78.60 - Last mean reward per episode: 78.60\n",
            "Num timesteps: 8920\n",
            "Best mean reward: 78.60 - Last mean reward per episode: 78.60\n",
            "Num timesteps: 8940\n",
            "Best mean reward: 78.60 - Last mean reward per episode: 78.60\n",
            "Num timesteps: 8960\n",
            "Best mean reward: 78.60 - Last mean reward per episode: 78.60\n",
            "Num timesteps: 8980\n",
            "Best mean reward: 78.60 - Last mean reward per episode: 78.60\n",
            "Num timesteps: 9000\n",
            "Best mean reward: 78.60 - Last mean reward per episode: 78.60\n",
            "Num timesteps: 9020\n",
            "Best mean reward: 78.60 - Last mean reward per episode: 78.60\n",
            "Num timesteps: 9040\n",
            "Best mean reward: 78.60 - Last mean reward per episode: 78.60\n",
            "Num timesteps: 9060\n",
            "Best mean reward: 78.60 - Last mean reward per episode: 80.50\n",
            "Saving new best model at 9050 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 9080\n",
            "Best mean reward: 80.50 - Last mean reward per episode: 80.50\n",
            "Num timesteps: 9100\n",
            "Best mean reward: 80.50 - Last mean reward per episode: 80.50\n",
            "Num timesteps: 9120\n",
            "Best mean reward: 80.50 - Last mean reward per episode: 80.50\n",
            "Num timesteps: 9140\n",
            "Best mean reward: 80.50 - Last mean reward per episode: 80.50\n",
            "Num timesteps: 9160\n",
            "Best mean reward: 80.50 - Last mean reward per episode: 81.24\n",
            "Saving new best model at 9144 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 9180\n",
            "Best mean reward: 81.24 - Last mean reward per episode: 81.24\n",
            "Num timesteps: 9200\n",
            "Best mean reward: 81.24 - Last mean reward per episode: 81.24\n",
            "Num timesteps: 9220\n",
            "Best mean reward: 81.24 - Last mean reward per episode: 81.24\n",
            "Num timesteps: 9240\n",
            "Best mean reward: 81.24 - Last mean reward per episode: 81.24\n",
            "Num timesteps: 9260\n",
            "Best mean reward: 81.24 - Last mean reward per episode: 81.24\n",
            "Num timesteps: 9280\n",
            "Best mean reward: 81.24 - Last mean reward per episode: 82.10\n",
            "Saving new best model at 9263 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 9300\n",
            "Best mean reward: 82.10 - Last mean reward per episode: 82.10\n",
            "Num timesteps: 9320\n",
            "Best mean reward: 82.10 - Last mean reward per episode: 82.10\n",
            "Num timesteps: 9340\n",
            "Best mean reward: 82.10 - Last mean reward per episode: 82.10\n",
            "Num timesteps: 9360\n",
            "Best mean reward: 82.10 - Last mean reward per episode: 82.89\n",
            "Saving new best model at 9355 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 9380\n",
            "Best mean reward: 82.89 - Last mean reward per episode: 82.89\n",
            "Num timesteps: 9400\n",
            "Best mean reward: 82.89 - Last mean reward per episode: 82.89\n",
            "Num timesteps: 9420\n",
            "Best mean reward: 82.89 - Last mean reward per episode: 82.89\n",
            "Num timesteps: 9440\n",
            "Best mean reward: 82.89 - Last mean reward per episode: 82.89\n",
            "Num timesteps: 9460\n",
            "Best mean reward: 82.89 - Last mean reward per episode: 82.89\n",
            "Num timesteps: 9480\n",
            "Best mean reward: 82.89 - Last mean reward per episode: 83.81\n",
            "Saving new best model at 9463 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 9500\n",
            "Best mean reward: 83.81 - Last mean reward per episode: 83.81\n",
            "Num timesteps: 9520\n",
            "Best mean reward: 83.81 - Last mean reward per episode: 83.81\n",
            "Num timesteps: 9540\n",
            "Best mean reward: 83.81 - Last mean reward per episode: 83.81\n",
            "Num timesteps: 9560\n",
            "Best mean reward: 83.81 - Last mean reward per episode: 83.81\n",
            "Num timesteps: 9580\n",
            "Best mean reward: 83.81 - Last mean reward per episode: 84.79\n",
            "Saving new best model at 9574 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 9600\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9620\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9640\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9660\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9680\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9700\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9720\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9740\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9760\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9780\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9800\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9820\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9840\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9860\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9880\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 84.79\n",
            "Num timesteps: 9900\n",
            "Best mean reward: 84.79 - Last mean reward per episode: 87.40\n",
            "Saving new best model at 9882 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 9920\n",
            "Best mean reward: 87.40 - Last mean reward per episode: 87.40\n",
            "Num timesteps: 9940\n",
            "Best mean reward: 87.40 - Last mean reward per episode: 87.40\n",
            "Num timesteps: 9960\n",
            "Best mean reward: 87.40 - Last mean reward per episode: 87.40\n",
            "Num timesteps: 9980\n",
            "Best mean reward: 87.40 - Last mean reward per episode: 87.40\n",
            "Num timesteps: 10000\n",
            "Best mean reward: 87.40 - Last mean reward per episode: 87.40\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.a2c.a2c.A2C at 0x7f93c8ddd610>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx18FkEORUn3"
      },
      "source": [
        "## Second example: Realtime plotting of performance\n",
        "While training, it is sometimes useful to how the training progresses over time, relative to the episodic reward.\n",
        "For this, Stable-Baselines has [Tensorboard support](https://stable-baselines.readthedocs.io/en/master/guide/tensorboard.html), however this can be very combersome, especially in disk space usage. \n",
        "\n",
        "**NOTE: Unfortunately live plotting does not work out of the box on google colab**\n",
        "\n",
        "Here, we can use callback again, to plot the episodic reward in realtime, using the monitoring wrapper:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0Bu1HWKRUn4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "b17af23a-e61e-494e-f35c-3f9e4ddff0fb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib notebook\n",
        "\n",
        "\n",
        "class PlottingCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for plotting the performance in realtime.\n",
        "\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, verbose=1):\n",
        "        super(PlottingCallback, self).__init__(verbose)\n",
        "        self._plot = None\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # get the monitor's data\n",
        "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
        "        if self._plot is None:  # make the plot\n",
        "            plt.ion()\n",
        "            fig = plt.figure(figsize=(6,3))\n",
        "            ax = fig.add_subplot(111)\n",
        "            line, = ax.plot(x, y)\n",
        "            self._plot = (line, ax, fig)\n",
        "            plt.show()\n",
        "        else: # update and rescale the plot\n",
        "            self._plot[0].set_data(x, y)\n",
        "            self._plot[-2].relim()\n",
        "            self._plot[-2].set_xlim([self.locals[\"total_timesteps\"] * -0.02, \n",
        "                                    self.locals[\"total_timesteps\"] * 1.02])\n",
        "            self._plot[-2].autoscale_view(True,True,True)\n",
        "            self._plot[-1].canvas.draw()\n",
        "        \n",
        "# Create log dir\n",
        "log_dir = \"/tmp/gym/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = make_vec_env('MountainCarContinuous-v0', n_envs=1, monitor_dir=log_dir)\n",
        "\n",
        "plotting_callback = PlottingCallback()\n",
        "        \n",
        "model = PPO('MlpPolicy', env, verbose=0)\n",
        "model.learn(20000, callback=plotting_callback)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-83233eaa202f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplotting_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;31m# Give access to local variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_locals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36mon_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-83233eaa202f>\u001b[0m in \u001b[0;36m_on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m                                     self.locals[\"total_timesteps\"] * 1.02])\n\u001b[1;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoscale_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Create log dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_webagg_core.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_png_is_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Swap the frames.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    392\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1736\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2628\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2630\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0;31m# the actual bbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_label_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2019\u001b[0m         \u001b[0;31m# get bounding boxes for this axis and any siblings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0;31m# that have been set by `fig.align_xlabels()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m         \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_boxes_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2003\u001b[0m         \u001b[0;31m# if we want to align labels from other axes:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2005\u001b[0;31m             \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2006\u001b[0m             \u001b[0mtlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks_to_draw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m             \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mticks\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdrawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \"\"\"\n\u001b[0;32m-> 1103\u001b[0;31m         \u001b[0mmajor_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_majorticklocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0mmajor_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0mmajor_ticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_majorticklocs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_majorticklocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;34m\"\"\"Get the array of major tick locations in data coordinates.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_minorticklocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_view_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtick_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36mtick_values\u001b[0;34m(self, vmin, vmax)\u001b[0m\n\u001b[1;32m   2209\u001b[0m         vmin, vmax = mtransforms.nonsingular(\n\u001b[1;32m   2210\u001b[0m             vmin, vmax, expander=1e-13, tiny=1e-14)\n\u001b[0;32m-> 2211\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2213\u001b[0m         \u001b[0mprune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36m_raw_ticks\u001b[0;34m(self, vmin, vmax)\u001b[0m\n\u001b[1;32m   2192\u001b[0m             \u001b[0mlow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_vmin\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbest_vmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2193\u001b[0m             \u001b[0mhigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_vmax\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbest_vmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2194\u001b[0;31m             \u001b[0mticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbest_vmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2195\u001b[0m             \u001b[0;31m# Count only the ticks that will be displayed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m             \u001b[0mnticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0m_vmax\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mticks\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_vmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49RVX7ieRUn7"
      },
      "source": [
        "## Third example: Progress bar\n",
        "Quality of life improvement are always welcome when developping and using RL. Here, we used [tqdm](https://tqdm.github.io/) to show a progress bar of the training, along with number of timesteps per second and the estimated time remaining to the end of the training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXa8f6FsRUn8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "fb110a7f43cc4e39ba9872d0e61f9c40",
            "f77546bde6cc492d83388baef3c86c71",
            "e3b9eecaf1a947eba243cace4ae06944",
            "a638279796f34e009997cae87b3dedd3",
            "5d68639747f44d48a8f7e367791d1753",
            "4e3c323f13d14c44be6cdf08c2931edb",
            "a75006590f074b6390e17ecb82991155",
            "a1f2ad0c12d6481a80a62c6af16aeb7c"
          ]
        },
        "outputId": "8862bdcf-357a-4e3f-9760-e8d61c52b64b"
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "class ProgressBarCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    :param pbar: (tqdm.pbar) Progress bar object\n",
        "    \"\"\"\n",
        "    def __init__(self, pbar):\n",
        "        super(ProgressBarCallback, self).__init__()\n",
        "        self._pbar = pbar\n",
        "\n",
        "    def _on_step(self):\n",
        "        # Update the progress bar:\n",
        "        self._pbar.n = self.num_timesteps\n",
        "        self._pbar.update(0)\n",
        "\n",
        "# this callback uses the 'with' block, allowing for correct initialisation and destruction\n",
        "class ProgressBarManager(object):\n",
        "    def __init__(self, total_timesteps): # init object with total timesteps\n",
        "        self.pbar = None\n",
        "        self.total_timesteps = total_timesteps\n",
        "        \n",
        "    def __enter__(self): # create the progress bar and callback, return the callback\n",
        "        self.pbar = tqdm(total=self.total_timesteps)\n",
        "            \n",
        "        return ProgressBarCallback(self.pbar)\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb): # close the callback\n",
        "        self.pbar.n = self.total_timesteps\n",
        "        self.pbar.update(0)\n",
        "        self.pbar.close()\n",
        "        \n",
        "model = TD3('MlpPolicy', 'Pendulum-v0', verbose=0)\n",
        "with ProgressBarManager(2000) as callback: # this the garanties that the tqdm progress bar closes correctly\n",
        "    model.learn(2000, callback=callback)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb110a7f43cc4e39ba9872d0e61f9c40",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBF4ij46RUoC"
      },
      "source": [
        "## Forth example: Composition\n",
        "Thanks to the functional nature of callbacks, it is possible to do a composition of callbacks, into a single callback. This means we can auto save our best model, show the progess bar and episodic reward of the training.\n",
        "\n",
        "The callbacks are automatically composed when you pass a list to the `learn()` method. Under the hood, a `CallbackList` is created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hU3T9tkRUoD",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746,
          "referenced_widgets": [
            "d5a9716bf2084a5293d36b0b37377340",
            "e3913d5e71b94527b9075ac7ef3f4be8",
            "3e6d9e5d3630412798bd38e6b520bdc7",
            "6cab26d2a3084ad299d7d1ef3c0e2562",
            "c6fb6368319346b082995d4e92f90bca",
            "1d73e195517146b18fd6113dde1dee44",
            "bdaa7410d3cd4d9e9bcc234ef41defc5",
            "9782cfdd1b6e482c98367413f87b9b73"
          ]
        },
        "outputId": "68bba0c0-a674-4b73-e3d9-b2780f1e2dbd"
      },
      "source": [
        "from stable_baselines3.common.callbacks import CallbackList\n",
        "\n",
        "# Create log dir\n",
        "log_dir = \"/tmp/gym/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = make_vec_env('CartPole-v1', n_envs=1, monitor_dir=log_dir)\n",
        "\n",
        "# Create callbacks\n",
        "auto_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
        "\n",
        "model = PPO('MlpPolicy', env, verbose=0)\n",
        "with ProgressBarManager(10000) as progress_callback:\n",
        "  # This is equivalent to callback=CallbackList([progress_callback, auto_save_callback])\n",
        "  model.learn(10000, callback=[progress_callback, auto_save_callback])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5a9716bf2084a5293d36b0b37377340",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num timesteps: 1000\n",
            "Best mean reward: -inf - Last mean reward per episode: 21.43\n",
            "Saving new best model at 986 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 2000\n",
            "Best mean reward: 21.43 - Last mean reward per episode: 21.88\n",
            "Saving new best model at 1991 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 3000\n",
            "Best mean reward: 21.88 - Last mean reward per episode: 23.76\n",
            "Saving new best model at 2987 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 4000\n",
            "Best mean reward: 23.76 - Last mean reward per episode: 26.12\n",
            "Saving new best model at 3960 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 5000\n",
            "Best mean reward: 26.12 - Last mean reward per episode: 29.61\n",
            "Saving new best model at 4928 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 6000\n",
            "Best mean reward: 29.61 - Last mean reward per episode: 32.20\n",
            "Saving new best model at 5943 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 7000\n",
            "Best mean reward: 32.20 - Last mean reward per episode: 37.16\n",
            "Saving new best model at 6961 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 8000\n",
            "Best mean reward: 37.16 - Last mean reward per episode: 42.46\n",
            "Saving new best model at 7963 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 9000\n",
            "Best mean reward: 42.46 - Last mean reward per episode: 48.56\n",
            "Saving new best model at 8958 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 10000\n",
            "Best mean reward: 48.56 - Last mean reward per episode: 53.44\n",
            "Saving new best model at 9759 timesteps\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRB4-qIxg_c9"
      },
      "source": [
        "## Exercise: Code your own callback\n",
        "\n",
        "\n",
        "The previous examples showed the basics of what is a callback and what you do with it.\n",
        "\n",
        "The goal of this exercise is to create a callback that will evaluate the model using a test environment and save it if this is the best known model.\n",
        "\n",
        "To make things easier, we are going to use a class instead of a function with the magic method `__call__`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOn0Sr3OhC2U"
      },
      "source": [
        "class EvalCallback(BaseCallback):\n",
        "  \"\"\"\n",
        "  Callback for evaluating an agent.\n",
        "  \n",
        "  :param eval_env: (gym.Env) The environment used for initialization\n",
        "  :param n_eval_episodes: (int) The number of episodes to test the agent\n",
        "  :param eval_freq: (int) Evaluate the agent every eval_freq call of the callback.\n",
        "  \"\"\"\n",
        "  def __init__(self, eval_env, n_eval_episodes=5, eval_freq=20):\n",
        "    super(EvalCallback, self).__init__()\n",
        "    self.eval_env = eval_env\n",
        "    self.n_eval_episodes = n_eval_episodes\n",
        "    self.eval_freq = eval_freq\n",
        "    self.best_mean_reward = -np.inf\n",
        "  \n",
        "  def _on_step(self):\n",
        "    \"\"\"\n",
        "    This method will be called by the model.\n",
        "\n",
        "    :return: (bool)\n",
        "    \"\"\"\n",
        "    \n",
        "    # self.n_calls is automatically updated because\n",
        "    # we derive from BaseCallback\n",
        "    if self.n_calls % self.eval_freq == 0:\n",
        "      # === YOUR CODE HERE ===#\n",
        "      # Evaluate the agent:\n",
        "      # you need to do self.n_eval_episodes loop using self.eval_env\n",
        "      # hint: you can use self.model.predict(obs, deterministic=True)\n",
        "      \n",
        "      # Save the agent if needed\n",
        "      # and update self.best_mean_reward\n",
        "      \n",
        "      print(\"Best mean reward: {:.2f}\".format(self.best_mean_reward))\n",
        "      \n",
        "\n",
        "      # ====================== #    \n",
        "    return True"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO0I81jAkQ0z"
      },
      "source": [
        "### Test your callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OMop3TlkTbx"
      },
      "source": [
        "# Env used for training\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "# Env for evaluating the agent\n",
        "eval_env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# === YOUR CODE HERE ===#\n",
        "# Create the callback object\n",
        "callback = EvalCallback()\n",
        "\n",
        "# Create the RL model\n",
        "model = TD3('MlpPolicy', 'CartPole-v1', verbose=0)\n",
        "\n",
        "# ====================== #\n",
        "\n",
        "# Train the RL model\n",
        "model.learn(int(100000), callback=callback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wS20a_NfMAh"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "\n",
        "In this notebook we have seen:\n",
        "- that good hyperparameters are key to the success of RL, you should not except the default ones to work on every problems\n",
        "- what is a callback and what you can do with it\n",
        "- how to create your own callback\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA4gCDtogIaD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}