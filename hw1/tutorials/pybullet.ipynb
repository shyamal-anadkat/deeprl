{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pybullet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Stable Baselines3 - PyBullet: Normalizing Features and Reward\n",
        "\n",
        "Github Repo: [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
        "\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
        "\n",
        "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)\n",
        "\n",
        "Pybullet source code: https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/\n",
        "\n",
        "## Install Dependencies and Stable Baselines Using Pip\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5286901a-24a5-44bf-a917-356a3408a07c"
      },
      "source": [
        "!pip install stable-baselines3[extra] pybullet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed pybullet-3.2.0 stable-baselines3-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Import policy, RL agent, Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "source": [
        "import os \n",
        "\n",
        "import pybullet_envs\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import VecNormalize"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c8VHsiXC7dL"
      },
      "source": [
        "## Create and wrap the environment with `VecNormalize`\n",
        "\n",
        "Normalizing input features may be essential to successful training of an RL agent (by default, images are scaled but not other types of input), for instance when training on [PyBullet](https://github.com/bulletphysics/bullet3/) environments. For that, a wrapper exists and will compute a running average and standard deviation of input features (it can do the same for rewards).\n",
        "\n",
        "More information about `VecNormalize`:\n",
        "- [Documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#stable_baselines3.common.vec_env.VecNormalize)\n",
        "- [Discussion](https://github.com/hill-a/stable-baselines/issues/698)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmxIq5UeC3Nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433cc19c-66e4-4de4-e725-63026123385e"
      },
      "source": [
        "env = make_vec_env(\"HalfCheetahBulletEnv-v0\", n_envs=1)\n",
        "\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxUMGsl5mabF"
      },
      "source": [
        "### Train the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQmsSZUHKNRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8905cf73-c8cf-4ad2-9ce3-519c0cfe6f86"
      },
      "source": [
        "model = PPO('MlpPolicy', env, verbose=1)\n",
        "model.learn(total_timesteps=2000)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -1.23e+03 |\n",
            "| time/              |           |\n",
            "|    fps             | 313       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 6         |\n",
            "|    total_timesteps | 2048      |\n",
            "----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7fc079d72090>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZYBIVoLmcR4"
      },
      "source": [
        "###Â Save the agent and the normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpMDXP0vmezv"
      },
      "source": [
        "# Don't forget to save the VecNormalize statistics when saving the agent\n",
        "log_dir = \"/tmp/\"\n",
        "model.save(log_dir + \"ppo_halfcheetah\")\n",
        "stats_path = os.path.join(log_dir, \"vec_normalize.pkl\")\n",
        "env.save(stats_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eezphIrRmr-Y"
      },
      "source": [
        "### Test model: load the saved agent and normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQT1k7lWmmTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865fbb33-9e7f-4529-8d62-f038f65a6853"
      },
      "source": [
        "# Load the agent\n",
        "model = PPO.load(log_dir + \"ppo_halfcheetah\")\n",
        "\n",
        "# Load the saved statistics\n",
        "env = make_vec_env(\"HalfCheetahBulletEnv-v0\", n_envs=1)\n",
        "env = VecNormalize.load(stats_path, env)\n",
        "#  do not update them at test time\n",
        "env.training = False\n",
        "# reward normalization is not needed at test time\n",
        "env.norm_reward = False"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voCxMxfYm2yc"
      },
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zC2IMJUm3Kw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a86b343-abd1-4560-c57a-ecc14e781c99"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward = -1218.67 +/- 93.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tHTCBIynFhd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}